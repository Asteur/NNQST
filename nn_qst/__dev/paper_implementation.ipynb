{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\bf{\\sigma_z}$ as reference basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lambda': {'W': array([[ 0.12995647,  0.60859445,  0.03596888],\n",
       "         [ 0.78243525,  0.10541418,  0.28989045]]),\n",
       "  'b': array([ 0.71262189,  0.13512335,  0.56960176]),\n",
       "  'c': array([ 0.7407072 ,  0.58162135])},\n",
       " 'mu': {'W': array([[ 0.37604412,  0.51080663,  0.41208797],\n",
       "         [ 0.76828467,  0.23351423,  0.14828215]]),\n",
       "  'b': array([ 0.84203523,  0.82530931,  0.06916255]),\n",
       "  'c': array([ 0.85793757,  0.67202164])}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vis = 3  # Number of visible neurons.\n",
    "n_hid = 2  # Number of hidden neurons.\n",
    "ratio = n_hid / n_vis  # Expressive power.\n",
    "\n",
    "W = np.random.random((n_hid, n_vis))  # Weights matrix.\n",
    "b = np.random.random(n_vis)  # Biases to visible neurons.\n",
    "c = np.random.random(n_hid)  # Biases to hidden neurons.\n",
    "\n",
    "params = {\n",
    "    \"lambda\": {\n",
    "        \"W\": np.random.random((n_hid, n_vis)),\n",
    "        \"b\": np.random.random(n_vis),\n",
    "        \"c\": np.random.random(n_hid)\n",
    "    },\n",
    "\n",
    "    \"mu\": {\n",
    "        \"W\": np.random.random((n_hid, n_vis)),\n",
    "        \"b\": np.random.random(n_vis),\n",
    "        \"c\": np.random.random(n_hid)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(ratio)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def boltzmann_joint_distribution(vis, hid, weights, b_bias, c_bias):\n",
    "    \"\"\"\n",
    "    (6) formula from original paper.\n",
    "    \"\"\"\n",
    "    tot = hid.dot(weights).dot(vis)\n",
    "    tot += vis.dot(b_bias)\n",
    "    tot += hid.dot(c_bias)\n",
    "    return np.exp(tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2647848363614869"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis = np.array([1,0,0])\n",
    "hid = np.array([0,1])\n",
    "\n",
    "boltzmann_joint_distribution(vis, hid, W, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All possible variants of bit arrays.\n",
    "import itertools\n",
    "\n",
    "\n",
    "def get_ideal_state(n=n_vis):\n",
    "    ideal_state = np.ones(n) / np.sqrt(n)\n",
    "    return ideal_state\n",
    "\n",
    "\n",
    "def get_all_states(n=n_vis):\n",
    "    all_states = np.array(list(map(np.array, itertools.product([0, 1], repeat=n))))[1:]\n",
    "    all_states = np.array([np.sqrt(x / x.sum()) for x in all_states])\n",
    "    return all_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fidelity(state1, state2):\n",
    "    return state1.dot(state2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**deprecated**\n",
    "\n",
    "We decided to sample $a_1, a_2, \\dots, a_n$ to build state $|\\psi_i>=a_1 |10\\dots0> + \\ldots + a_n |0\\dots01>$ such that $a_1^2 + \\ldots + a_n^2 = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_a(size, n=n_vis): \n",
    "#     res = []\n",
    "#     for i in range(size):\n",
    "#         tmp = np.random.random(n)\n",
    "#         tmp = tmp / tmp.sum()\n",
    "#         tmp = np.array([np.sqrt(x) for x in tmp])\n",
    "#         res.append(tmp)\n",
    "    \n",
    "    res = np.random.choice([0, 1], (size, n))\n",
    "    res = np.array(list(filter(lambda x: x.sum() > 0, res)))\n",
    "    \n",
    "    tmp = np.sqrt(np.diag(res.dot(res.T)))\n",
    "    tmp = tmp.reshape(len(tmp), -1)\n",
    "    \n",
    "    res = res / tmp\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.70710678,  0.70710678],\n",
       "       [ 0.        ,  0.70710678,  0.70710678],\n",
       "       [ 0.        ,  1.        ,  0.        ],\n",
       "       [ 0.70710678,  0.        ,  0.70710678],\n",
       "       [ 0.        ,  0.        ,  1.        ],\n",
       "       [ 0.        ,  1.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  1.        ],\n",
       "       [ 0.70710678,  0.        ,  0.70710678],\n",
       "       [ 0.        ,  1.        ,  0.        ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_a(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 1, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([0, 1], (10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_dataset(size, dimens=n_vis, num_samples=1000, num_of_bins=20, verbose=False):\n",
    "    \"\"\"\n",
    "    size # Dataset size.\n",
    "    num_samples = 1000  # Number of sampled sets of `a`.\n",
    "    num_of_bins = 20\n",
    "    \n",
    "    \"\"\"\n",
    "    a_samples = sample_a(num_samples, dimens)\n",
    "    num_samples = len(a_samples)\n",
    "\n",
    "    fidelities = np.array([fidelity(x, ideal_state) for x in a_samples])\n",
    "\n",
    "    probs = fidelities / sum(fidelities)\n",
    "    \n",
    "    if verbose:\n",
    "        plt.scatter(np.arange(num_samples), sorted(fidelities))\n",
    "        plt.title('Fidelity of sampled states - set of $\\overrightarrow{a}$')\n",
    "        plt.xlabel('Sample')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.hist(fidelities, bins=20)\n",
    "        plt.title(\"Distribution of fidelities\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.scatter(np.arange(num_samples), sorted(probs))\n",
    "        plt.title('Probability - fidelities / sum(fidelities)')\n",
    "        plt.show()\n",
    "        \n",
    "    eps = 1e-7\n",
    "    bins = np.linspace((1 - eps) * fidelities.min(), (1 + eps) * fidelities.max(), num_of_bins + 1)\n",
    "    bins_indices = np.digitize(fidelities, bins)\n",
    "    \n",
    "#     if verbose:\n",
    "#         plt.title('Distribution of bins indices')\n",
    "#         plt.hist(bins_indices)\n",
    "#         plt.show()\n",
    "        \n",
    "    uniq_vals = np.arange(1, num_of_bins + 1)\n",
    "    probs = np.array([(bins_indices == num).sum() for num in uniq_vals]) / num_samples\n",
    "    \n",
    "    if verbose:\n",
    "        plt.title('Probability - after binarization')\n",
    "        plt.plot(probs, '.')\n",
    "        plt.show()\n",
    "        \n",
    "    bins_sampled = np.random.choice(uniq_vals, p=probs, size=size)\n",
    "    \n",
    "    tmp = list(enumerate(bins_indices))\n",
    "\n",
    "    bins_states = dict()\n",
    "    for num in set(bins_indices):\n",
    "        bins_states[num] = np.array([i for i, x in tmp if x == num])\n",
    "        \n",
    "    res_a_sampled = []\n",
    "    for num in bins_sampled:\n",
    "        if num in bins_states:\n",
    "            res_a_sampled.append(a_samples[np.random.choice(bins_states[num])])\n",
    "    \n",
    "    res_a_sampled = np.array(res_a_sampled)\n",
    "            \n",
    "    if verbose:\n",
    "        plt.hist([fidelity(x, ideal_state) for x in res_a_sampled], bins=20)\n",
    "        plt.title(\"Distribution of fidelities - dataset\")\n",
    "        plt.show()\n",
    "        \n",
    "    return res_a_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset1 = sample_dataset(500, num_of_bins=3, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.9722352336107"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def boltzmann_margin_distribution(vis, weights, b_bias, c_bias):\n",
    "    \"\"\"\n",
    "    (7) formula from original paper.\n",
    "    \"\"\"\n",
    "    tmp = weights.dot(vis)\n",
    "    tmp += c_bias\n",
    "    tmp = np.exp(tmp)\n",
    "    tmp += 1\n",
    "    tmp = np.log(tmp)\n",
    "    tmp = tmp.sum()\n",
    "    tmp += vis.dot(b_bias)\n",
    "    return np.exp(tmp)\n",
    "\n",
    "boltzmann_margin_distribution(vis, W, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_k(k, vis, params):\n",
    "    \"\"\"\n",
    "    `k` is either \"lambda\" or \"mu\".\n",
    "    \"\"\"\n",
    "    return boltzmann_margin_distribution(vis, params[k]['W'],\n",
    "                                         params[k]['b'], params[k]['c'])\n",
    "\n",
    "def p_lambda(vis, params):\n",
    "    return p_k('lambda', vis, params)\n",
    "\n",
    "def p_mu(vis, params):\n",
    "    return p_k('mu', vis, params)\n",
    "\n",
    "def phi_mu(vis, params):\n",
    "    return np.log(p_mu(vis, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.851975048683016"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All possible variants of bit arrays.\n",
    "# import itertools\n",
    "# n = 3\n",
    "# lst = np.array(list(map(np.array, itertools.product([0, 1], repeat=n))))\n",
    "\n",
    "sigmas = np.eye(n_vis)\n",
    "\n",
    "# Normalization constant.\n",
    "Z_lambda = sum(list(map(lambda x: p_lambda(x), sigmas)))\n",
    "Z_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def psi_lambda_mu(vis):\n",
    "    \"\"\"\n",
    "    (8) formula from original paper.\n",
    "    \"\"\"\n",
    "    tmp = 1j * phi_mu(vis) / 2\n",
    "    tmp = np.exp(tmp)\n",
    "    tmp *= np.sqrt(p_lambda(vis) / Z_lambda)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1863799785817617+0.62303563240778093j)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_lambda_mu(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def quasi_prob(vis, u=None):\n",
    "    \"\"\"\n",
    "    (12) formula from original paper.\n",
    "    \"\"\"\n",
    "    tmp = 1j * phi_mu(vis) / 2\n",
    "    tmp = np.exp(tmp)\n",
    "    tmp *= np.sqrt(p_lambda(vis))\n",
    "    \n",
    "    if u:\n",
    "        return u * tmp\n",
    "    else:\n",
    "        return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def D_k_sigma(k, sigma):\n",
    "    \"\"\"\n",
    "    sigma (np.array)\n",
    "    \n",
    "    \"\"\"\n",
    "    weights = params[k]['W']\n",
    "    c_bias = params[k]['c']\n",
    "    \n",
    "    grad_b = sigma\n",
    "    \n",
    "    tmp = weights.dot(sigma)\n",
    "    tmp += c_bias\n",
    "    tmp = np.exp(tmp)\n",
    "    grad_c = tmp / (1 + tmp)\n",
    "    \n",
    "    grad_W = np.outer(grad_c, sigma)\n",
    "    \n",
    "    res = {\n",
    "        'W': grad_W,\n",
    "        'c': grad_c,\n",
    "        'b': grad_b\n",
    "    }\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': array([[ 0.63585126,  0.        ,  0.        ],\n",
       "        [ 0.51863706,  0.        ,  0.        ]]),\n",
       " 'b': array([1, 0, 0]),\n",
       " 'c': array([ 0.63585126,  0.51863706])}"
      ]
     },
     "execution_count": 756,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_k_sigma('lambda', vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averaged_D_k_Qb(k, batch):\n",
    "    \"\"\"\n",
    "    (15) formula from original paper.\n",
    "    \n",
    "    \"\"\"\n",
    "    res_W = None\n",
    "    res_b = None\n",
    "    res_c = None\n",
    "    quasi_probs = None\n",
    "    \n",
    "    for x in batch:\n",
    "        gradients = D_k_sigma(k, x)\n",
    "        quasi = quasi_prob(x)\n",
    "        \n",
    "        if quasi_probs is None:\n",
    "            quasi_probs = quasi\n",
    "            res_W = quasi * gradients[\"W\"]\n",
    "            res_b = quasi * gradients[\"b\"]\n",
    "            res_c = quasi * gradients[\"c\"]\n",
    "        else:\n",
    "            quasi_probs += quasi\n",
    "            res_W += quasi * gradients[\"W\"]\n",
    "            res_b += quasi * gradients[\"b\"]\n",
    "            res_c += quasi * gradients[\"c\"]\n",
    "    \n",
    "    res_W /= quasi_probs\n",
    "    res_b /= quasi_probs\n",
    "    res_c /= quasi_probs\n",
    "    \n",
    "    res = {\n",
    "        'W': res_W,\n",
    "        'c': res_c,\n",
    "        'b': res_b\n",
    "    }\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = averaged_D_k_Qb('lambda', dataset1)['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averaged_D_lambda_p_lambda(batch):\n",
    "    \"\"\"\n",
    "    (17) formula from original paper.\n",
    "    \n",
    "    \"\"\"\n",
    "    n = len(batch)\n",
    "    \n",
    "    res_W = None\n",
    "    res_b = None\n",
    "    res_c = None\n",
    "    \n",
    "    for x in batch:\n",
    "        gradients = D_k_sigma('lambda', x)\n",
    "        \n",
    "        if res_W is None:\n",
    "            res_W = gradients[\"W\"]\n",
    "            res_b = gradients[\"b\"]\n",
    "            res_c = gradients[\"c\"]\n",
    "        else:\n",
    "            res_W += gradients[\"W\"]\n",
    "            res_b += gradients[\"b\"]\n",
    "            res_c += gradients[\"c\"]\n",
    "            \n",
    "    res_W /= n\n",
    "    res_b /= n\n",
    "    res_c /= n\n",
    "    \n",
    "    res = {\n",
    "        'W': res_W,\n",
    "        'c': res_c,\n",
    "        'b': res_b\n",
    "    }\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': array([[ 0.38757272,  0.3851625 ,  0.44103962],\n",
       "        [ 0.33641119,  0.33435325,  0.38344697]]),\n",
       " 'b': array([ 0.53687518,  0.53333991,  0.60643279]),\n",
       " 'c': array([ 0.7243129,  0.6291669])}"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averaged_D_lambda_p_lambda(dataset1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_lambda_div(dataset, N_b=1):\n",
    "    assert len(dataset) == N_b, \"Number of bases must be equal to length of density measurements dataset\"\n",
    "    \n",
    "    for b in range(N_b):\n",
    "        len(dataset[b])\n",
    "        tmp = 0\n",
    "        for sigma in dataset[b]:\n",
    "            tmp += averaged_D_k_Qb('lambda', sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using defined distribution we can assign probability to every outcome and after that we can sample some configuration $\\bf{\\sigma}$.\n",
    "\n",
    "Better: perform Gibbs sampling as in usual RBM using two conditional distributions $p_{\\lambda}(\\sigma\\vert h)$ and $p_{\\lambda}(h\\vert \\sigma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Series of data sets $D_b$ for each base $b=1,\\dots, N_b - 1$\n",
    "\n",
    "$U_b(\\sigma, \\sigma^{[b]})$ - basis transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_b = 2 # Number of bases.\n",
    "dataset = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset[0] = np.array([[1,0,0],\n",
    "                       [0,0,1]])\n",
    "\n",
    "dataset[1] = np.array([[1,0,0]])\n",
    "\n",
    "assert len(dataset) == N_b, \"Number of bases must be equal to size of dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def total_divergence():\n",
    "    tot = N_b * np.log(Z_lambda)\n",
    "    \n",
    "    return tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.4343779095210216"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_divergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
